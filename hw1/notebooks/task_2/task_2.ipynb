{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d838f4-1019-466c-adc2-63110d330263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CLASSPATH'] = '/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df221e92-9423-48c0-9124-358fa84514fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import fs\n",
    "hdfs = fs.HadoopFileSystem(\"namenode\", 8020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7e43d8-239f-4c4f-8086-0b56e593c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STARWARS_FOLDER = \"/star_wars\"\n",
    "hdfs.create_dir(STARWARS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746a96a4-9fe1-4960-a744-1f716bf1f685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../SW_EpisodeV.txt', '../SW_EpisodeIV.txt', '../SW_EpisodeVI.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "star_wars_files = []\n",
    "for file in os.listdir(\"../\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        star_wars_files.append(\"../\"+file)\n",
    "star_wars_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c88922-01e9-41f7-81fe-bb8d80e70b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for star_wars_file in star_wars_files:\n",
    "    with open(star_wars_file, \"rb\") as f:\n",
    "        text = f.read()\n",
    "    with hdfs.open_output_stream(f\"{STARWARS_FOLDER}/{star_wars_file}\") as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c1f191-6e37-49fc-b4b4-df02f7c97164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest_replic.py\n"
     ]
    }
   ],
   "source": [
    "%%file longest_replic.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "\n",
    "class MRSongCount(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_first,\n",
    "                   reducer=self.reducer_first),\n",
    "            MRStep(reducer=self.reducer_second)\n",
    "        ]\n",
    "\n",
    "    def mapper_first(self, _, text):\n",
    "        lines = text.split(\"\\n\")\n",
    "        character_max_replic = defaultdict(str)\n",
    "        for line in lines:\n",
    "            # skip head lines\n",
    "            if len(line.split('\" \"')) != 3:\n",
    "                continue\n",
    "            _, character, replic = line[:-1].split('\" \"')\n",
    "            if len(character_max_replic[character]) < len(replic):\n",
    "                character_max_replic[character] = replic\n",
    "\n",
    "        for char, replic in character_max_replic.items():\n",
    "            yield char, replic\n",
    "\n",
    "    def reducer_first(self, character, values):\n",
    "        character_max_replic = defaultdict(str)\n",
    "        for replic in values:\n",
    "            if len(character_max_replic[character]) < len(replic):\n",
    "                character_max_replic[character] = replic\n",
    "\n",
    "        for char, replic in character_max_replic.items():\n",
    "            yield None, (char, replic)\n",
    "\n",
    "    def reducer_second(self, _, values):\n",
    "        for key, val in sorted(values, key=lambda x: len(x[1])):\n",
    "            yield key, val\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRSongCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8919e2e4-f04f-4558-b574-0bbfd6b16668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/longest_replic.root.20231123.205805.062555\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205805.062555/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205805.062555/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8845835771086635164/] [] /tmp/streamjob573596746858960106.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700771040558_0019\n",
      "  Total input files to process : 3\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1700771040558_0019\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1700771040558_0019\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1700771040558_0019/\n",
      "  Running job: job_1700771040558_0019\n",
      "  Job job_1700771040558_0019 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1700771040558_0019 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205805.062555/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=181941\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=17103\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=172590\n",
      "\t\tFILE: Number of bytes written=1476785\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=182207\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=17103\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11141120\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2458624\n",
      "\t\tTotal time spent by all map tasks (ms)=10880\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21760\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2401\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4802\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10880\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2401\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3590\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=300\n",
      "\t\tInput split bytes=266\n",
      "\t\tMap input records=2526\n",
      "\t\tMap output bytes=167298\n",
      "\t\tMap output materialized bytes=172602\n",
      "\t\tMap output records=2523\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPeak Map Physical memory (bytes)=311259136\n",
      "\t\tPeak Map Virtual memory (bytes)=2563805184\n",
      "\t\tPeak Reduce Physical memory (bytes)=220463104\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2568925184\n",
      "\t\tPhysical memory (bytes) snapshot=1150218240\n",
      "\t\tReduce input groups=129\n",
      "\t\tReduce input records=2523\n",
      "\t\tReduce output records=129\n",
      "\t\tReduce shuffle bytes=172602\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=5046\n",
      "\t\tTotal committed heap usage (bytes)=1180172288\n",
      "\t\tVirtual memory (bytes) snapshot=10258288640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8769041241944147738/] [] /tmp/streamjob1900775961023409099.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700771040558_0020\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1700771040558_0020\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1700771040558_0020\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1700771040558_0020/\n",
      "  Running job: job_1700771040558_0020\n",
      "  Job job_1700771040558_0020 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1700771040558_0020 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205805.062555/output\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21199\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=16071\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17469\n",
      "\t\tFILE: Number of bytes written=883268\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21515\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=16071\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5299200\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2354176\n",
      "\t\tTotal time spent by all map tasks (ms)=5175\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10350\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2299\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4598\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5175\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2299\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2060\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=204\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=129\n",
      "\t\tMap output bytes=17154\n",
      "\t\tMap output materialized bytes=17475\n",
      "\t\tMap output records=129\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=308363264\n",
      "\t\tPeak Map Virtual memory (bytes)=2562465792\n",
      "\t\tPeak Reduce Physical memory (bytes)=255496192\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2566119424\n",
      "\t\tPhysical memory (bytes) snapshot=867975168\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=129\n",
      "\t\tReduce output records=129\n",
      "\t\tReduce shuffle bytes=17475\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=258\n",
      "\t\tTotal committed heap usage (bytes)=930086912\n",
      "\t\tVirtual memory (bytes) snapshot=7688720384\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205805.062555/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205805.062555/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205805.062555...\n",
      "Removing temp directory /tmp/longest_replic.root.20231123.205805.062555...\n"
     ]
    }
   ],
   "source": [
    "!rm out_all\n",
    "!python3 longest_replic.py -r hadoop hdfs://namenode:8020/SW_EpisodeIV.txt hdfs://namenode:8020/SW_EpisodeV.txt hdfs://namenode:8020/SW_EpisodeVI.txt > out_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7496430-2b5d-47b4-8ba5-cc3e2f3433d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/longest_replic.root.20231123.205910.252705\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205910.252705/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205910.252705/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6273611080265007727/] [] /tmp/streamjob4846912479678049318.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700771040558_0021\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1700771040558_0021\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1700771040558_0021\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1700771040558_0021/\n",
      "  Running job: job_1700771040558_0021\n",
      "  Job job_1700771040558_0021 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1700771040558_0021 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205910.252705/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=82374\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9064\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=74563\n",
      "\t\tFILE: Number of bytes written=997591\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=82552\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=9064\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5468160\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2364416\n",
      "\t\tTotal time spent by all map tasks (ms)=5340\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10680\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2309\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4618\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5340\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2309\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2250\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=195\n",
      "\t\tInput split bytes=178\n",
      "\t\tMap input records=1011\n",
      "\t\tMap output bytes=72416\n",
      "\t\tMap output materialized bytes=74569\n",
      "\t\tMap output records=1010\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=268353536\n",
      "\t\tPeak Map Virtual memory (bytes)=2562629632\n",
      "\t\tPeak Reduce Physical memory (bytes)=255590400\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2568040448\n",
      "\t\tPhysical memory (bytes) snapshot=788557824\n",
      "\t\tReduce input groups=60\n",
      "\t\tReduce input records=1010\n",
      "\t\tReduce output records=60\n",
      "\t\tReduce shuffle bytes=74569\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2020\n",
      "\t\tTotal committed heap usage (bytes)=917504000\n",
      "\t\tVirtual memory (bytes) snapshot=7693246464\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6328656564664535347/] [] /tmp/streamjob7696327377402102144.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700771040558_0022\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1700771040558_0022\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1700771040558_0022\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1700771040558_0022/\n",
      "  Running job: job_1700771040558_0022\n",
      "  Job job_1700771040558_0022 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1700771040558_0022 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205910.252705/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=13160\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8584\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9254\n",
      "\t\tFILE: Number of bytes written=866838\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=13476\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=8584\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5542912\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2361344\n",
      "\t\tTotal time spent by all map tasks (ms)=5413\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10826\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2306\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4612\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5413\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2306\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2050\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=203\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=60\n",
      "\t\tMap output bytes=9096\n",
      "\t\tMap output materialized bytes=9260\n",
      "\t\tMap output records=60\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=313405440\n",
      "\t\tPeak Map Virtual memory (bytes)=2566422528\n",
      "\t\tPeak Reduce Physical memory (bytes)=216285184\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2568679424\n",
      "\t\tPhysical memory (bytes) snapshot=838692864\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=60\n",
      "\t\tReduce output records=60\n",
      "\t\tReduce shuffle bytes=9260\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=120\n",
      "\t\tTotal committed heap usage (bytes)=737673216\n",
      "\t\tVirtual memory (bytes) snapshot=7700779008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205910.252705/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205910.252705/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.205910.252705...\n",
      "Removing temp directory /tmp/longest_replic.root.20231123.205910.252705...\n"
     ]
    }
   ],
   "source": [
    "!rm -rf out_IV\n",
    "!python3 longest_replic.py -r hadoop hdfs://namenode:8020/SW_EpisodeIV.txt > out_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ede4ce7-d07a-4fb5-acda-f6efd4214b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/longest_replic.root.20231123.210012.076578\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210012.076578/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210012.076578/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3765610060139391229/] [] /tmp/streamjob2961289739118378498.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700771040558_0023\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1700771040558_0023\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1700771040558_0023\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1700771040558_0023/\n",
      "  Running job: job_1700771040558_0023\n",
      "  Job job_1700771040558_0023 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1700771040558_0023 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210012.076578/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=59583\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5573\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=52333\n",
      "\t\tFILE: Number of bytes written=953128\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=59759\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=5573\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5721088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2349056\n",
      "\t\tTotal time spent by all map tasks (ms)=5587\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11174\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2294\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4588\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5587\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2294\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2170\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=193\n",
      "\t\tInput split bytes=176\n",
      "\t\tMap input records=840\n",
      "\t\tMap output bytes=50594\n",
      "\t\tMap output materialized bytes=52339\n",
      "\t\tMap output records=839\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=309637120\n",
      "\t\tPeak Map Virtual memory (bytes)=2561482752\n",
      "\t\tPeak Reduce Physical memory (bytes)=257458176\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2569125888\n",
      "\t\tPhysical memory (bytes) snapshot=876306432\n",
      "\t\tReduce input groups=49\n",
      "\t\tReduce input records=839\n",
      "\t\tReduce output records=49\n",
      "\t\tReduce shuffle bytes=52339\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1678\n",
      "\t\tTotal committed heap usage (bytes)=932708352\n",
      "\t\tVirtual memory (bytes) snapshot=7691943936\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3404580755317764350/] [] /tmp/streamjob3499974615353483501.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700771040558_0024\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1700771040558_0024\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1700771040558_0024\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1700771040558_0024/\n",
      "  Running job: job_1700771040558_0024\n",
      "  Job job_1700771040558_0024 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1700771040558_0024 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210012.076578/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8360\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5181\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5712\n",
      "\t\tFILE: Number of bytes written=859754\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8676\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=5181\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5111808\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2311168\n",
      "\t\tTotal time spent by all map tasks (ms)=4992\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9984\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2257\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4514\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4992\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2257\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2020\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=185\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=49\n",
      "\t\tMap output bytes=5590\n",
      "\t\tMap output materialized bytes=5718\n",
      "\t\tMap output records=49\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=267882496\n",
      "\t\tPeak Map Virtual memory (bytes)=2561695744\n",
      "\t\tPeak Reduce Physical memory (bytes)=219668480\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2569338880\n",
      "\t\tPhysical memory (bytes) snapshot=752672768\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=49\n",
      "\t\tReduce output records=49\n",
      "\t\tReduce shuffle bytes=5718\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=98\n",
      "\t\tTotal committed heap usage (bytes)=848822272\n",
      "\t\tVirtual memory (bytes) snapshot=7692627968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210012.076578/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210012.076578/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210012.076578...\n",
      "Removing temp directory /tmp/longest_replic.root.20231123.210012.076578...\n"
     ]
    }
   ],
   "source": [
    "!rm -rf out_V\n",
    "!python3 longest_replic.py -r hadoop hdfs://namenode:8020/SW_EpisodeV.txt > out_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b782ac-08b7-4e11-908a-86fbf03b28d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/longest_replic.root.20231123.210114.016828\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210114.016828/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210114.016828/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6250145096138030592/] [] /tmp/streamjob2020378464486749693.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700771040558_0025\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1700771040558_0025\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1700771040558_0025\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1700771040558_0025/\n",
      "  Running job: job_1700771040558_0025\n",
      "  Job job_1700771040558_0025 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1700771040558_0025 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210114.016828/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=52272\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6991\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=45706\n",
      "\t\tFILE: Number of bytes written=939877\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=52450\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=6991\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5800960\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2410496\n",
      "\t\tTotal time spent by all map tasks (ms)=5665\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11330\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2354\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4708\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5665\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2354\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=234\n",
      "\t\tInput split bytes=178\n",
      "\t\tMap input records=675\n",
      "\t\tMap output bytes=44288\n",
      "\t\tMap output materialized bytes=45712\n",
      "\t\tMap output records=674\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=312328192\n",
      "\t\tPeak Map Virtual memory (bytes)=2562359296\n",
      "\t\tPeak Reduce Physical memory (bytes)=213954560\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2567499776\n",
      "\t\tPhysical memory (bytes) snapshot=830291968\n",
      "\t\tReduce input groups=53\n",
      "\t\tReduce input records=674\n",
      "\t\tReduce output records=53\n",
      "\t\tReduce shuffle bytes=45712\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1348\n",
      "\t\tTotal committed heap usage (bytes)=770179072\n",
      "\t\tVirtual memory (bytes) snapshot=7692029952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar222815075738105997/] [] /tmp/streamjob5228530620026382050.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.19.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700771040558_0026\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1700771040558_0026\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1700771040558_0026\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1700771040558_0026/\n",
      "  Running job: job_1700771040558_0026\n",
      "  Job job_1700771040558_0026 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1700771040558_0026 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210114.016828/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=10487\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6567\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7147\n",
      "\t\tFILE: Number of bytes written=862624\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10803\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=6567\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5210112\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2314240\n",
      "\t\tTotal time spent by all map tasks (ms)=5088\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10176\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2260\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4520\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5088\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2260\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2140\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=193\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=53\n",
      "\t\tMap output bytes=7013\n",
      "\t\tMap output materialized bytes=7153\n",
      "\t\tMap output records=53\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=310530048\n",
      "\t\tPeak Map Virtual memory (bytes)=2565386240\n",
      "\t\tPeak Reduce Physical memory (bytes)=223408128\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2567979008\n",
      "\t\tPhysical memory (bytes) snapshot=798228480\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=53\n",
      "\t\tReduce output records=53\n",
      "\t\tReduce shuffle bytes=7153\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=106\n",
      "\t\tTotal committed heap usage (bytes)=857735168\n",
      "\t\tVirtual memory (bytes) snapshot=7696007168\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210114.016828/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210114.016828/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/longest_replic.root.20231123.210114.016828...\n",
      "Removing temp directory /tmp/longest_replic.root.20231123.210114.016828...\n"
     ]
    }
   ],
   "source": [
    "!rm -rf out_VI\n",
    "!python3 longest_replic.py -r hadoop hdfs://namenode:8020/SW_EpisodeVI.txt > out_VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6780b-d5e9-4b25-aa30-df757d191b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
